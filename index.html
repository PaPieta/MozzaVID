<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MozzaVID is a volumetric CT image dataset containing up to 37.824 scans of mozzarella cheese microstructure. It can serve as a benchmark for volumetric deep learning, and for advancing food structure analysis.">
  <meta property="og:title" content="MozzaVID: Mozzarella Volumetric Image Dataset"/>
  <meta property="og:description" content="Discover MozzaVID - one of the biggest volumetric benchmark datasets for deep learning, and explore the variety and complexity of mozzarella microstrucutre."/>
  <meta property="og:url" content="https://papieta.github.io/MozzaVID/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/sample_slices.png" />
  <meta property="og:image:width" content="1050"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MozzaVID: Mozzarella Volumetric Image Dataset">
  <meta name="twitter:description" content="Discover MozzaVID - one of the biggest volumetric benchmark datasets for deep learning, and explore the variety and complexity of mozzarella microstrucutre.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/sample_slices.png">
  <meta name="twitter:card" content="Mozzarella microstructure in example coarse and fine-grained classes.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="benchmark dataset, 3D, volumetric, deep learning, structural analysis, mozzarella">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MozzaVID: Mozzarella Volumetric Image Dataset</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MozzaVID: Mozzarella Volumetric Image Dataset</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://orcid.org/0009-0005-7634-6627" target="_blank">Pawel Tomasz Pieta</a>,</span>
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-0823-0316" target="_blank">Peter Winkel Rasmussen</a>,</span>
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-0068-8170" target="_blank">Anders Bjorholm Dahl</a></span>,
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-0603-3669" target="_blank">Jeppe Revall Frisvad</a></span>,
              <span class="author-block">
                <a href="https://orcid.org/0000-0003-2569-6473" target="_blank">Siavash Arjomand Bigdeli</a></span>,
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-2895-1882" target="_blank">Carsten Gundlach</a></span>,
              <span class="author-block">
                <a href="https://orcid.org/0000-0002-3668-3128" target="_blank">Anders Nymark Christensen</a></span>
              
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Technical University of Denmark, 
                      Kgs. Lyngby, Denmark<br>ArXiv 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                          <!-- Data link-->
                      <span class="link-block">
                        <a href="https://archive.compute.dtu.dk/files/public/projects/MozzaVID/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-download"></i>
                        </span>
                        <span>Data</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/PaPieta/MozzaVID/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/sample_slices.png" alt="MY ALT TEXT"/>
        
      <h2 class="subtitle has-text-centered">
        Mozzarella microstructure in example coarse and fine-grained classes. MozzaVID is a benchmark volumetric (3D) classification dataset containing mozzarella CT images at three dataset splits (sizes) and two classification targets.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Influenced by the complexity of volumetric imaging, there is a shortage of established datasets useful for benchmarking volumetric deep-learning models. As a consequence, new and existing models are not easily comparable, limiting the development of architectures optimized specifically for volumetric data. To counteract this trend, we introduce MozzaVID -- a large, clean, and versatile volumetric classification dataset. Our dataset contains X-ray computed tomography (CT) images of mozzarella microstructure and enables the classification of 25 cheese types and 149 cheese samples. We provide data in three different resolutions, resulting in three dataset instances containing from 591 to 37,824 images. While being general-purpose, the dataset also facilitates investigating mozzarella structure properties. The structure of food directly affects its functional properties and thus its consumption experience. Understanding food structure helps tune the production and mimicking it enables sustainable alternatives to animal-derived food products. The complex and disordered nature of food structures brings a unique challenge, where a choice of appropriate imaging method, scale, and sample size is not trivial. With this dataset we aim to address these complexities, contributing to more robust structural analysis models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <div class="columns is-centered">
          <img src="static/images/dataset_instance_creation.png" alt="Dataset instance preparation" style="height: 50vh"/>
          </div>
          <h2 class="subtitle has-text-centered">
            We use the 590 high-resolution synchrotron scans of the mozzarella structure to create three dataset instances. The raw volume is downscaled and split, ensuring that in each case the final volumes have the shape of 192 cubed. The three instances contain 591, 4.728, and 37.824 volumes, respectively.
          </h2>
      </div>
      <div class="item">
        <div class="columns is-centered">
          <!-- Your image here -->
          <img src="static/images/2d_vs_3d_size_v3.png" alt="2D and 3D dataset sizes" style="height: 45vh"/>
          </div>
          <h2 class="subtitle has-text-centered">
            Most benchmark 2D datasets consist of large amounts of relatively small images. In contrast, 3D volumetric datasets are scarce and often contain fewer, large volumes. The three instances of the proposed MozzaVID dataset form a bridge between the two groups while maintaining the volume sizes known from other volumetric datasets.
          </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/accuracy_table.png" alt="Tested models' accuracy"/>
        <h2 class="subtitle has-text-centered">
          The dataset is evaluated on a range of established deep-learning architectures. Reported classification accuracy highlights the impact of the 3D representation, as well as performance increase with the dataset size. 
          At the same time, Swin - the most advanced model performs very well on 2D data, while underperforming in 3D. This trend suggests that State-of-the-Art models are not optimized for volumetric data, highlighting the need for 3D-oriented model development. 
          We argue that MozzaVID can serve as a useful benchmark for this purpose. The Large dataset can be used to establish the base performance of the model, while the smaller instances allow for tailoring the models to the specific limitations of volumetric data.
        </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Text container -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Structural analysis</h2>
        <div class="content has-text-justified">
          <p>
            Apart from providing a general benchmark, MozzaVID is specifically targeted towards developing and evaluating methods for deep learning-based structural analysis. The structure of materials directly affects their functional properties, and structural analysis is an important use case for volumetric images. Structural properties are also an essential part of certain food products such as meat, bread, pastries, and cheese (e.g. mozzarella). With 34% of greenhouse gas emissions linked to food, understanding structural properties is crucial for developing environmentally-friendly alternatives of known structured foods that are also pleasant to eat.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End text container -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <div class="columns is-centered">
        <!-- Your image here -->
        <img src="static/images/exp_design_pca.png" alt="Colored PCA of sample experimental design" style="height: 45vh"/>
        </div>
        <h2 class="subtitle has-text-centered">
          We use the experimental design of the mozzarella types to express the similarity between classes. The parameter space is compressed with PCA and colored to enable further analysis.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/UMAP_coarse.png" alt="UMAP, coarse-grained target" />
        <h2 class="subtitle has-text-centered">
          We compress the second-to-last layer embeddings of our best-performing coarse-grained model (3D ResNet) using the UMAP algorithm. The resulting clusters closely follow the class assignment. At the same time, cheese types with similar experimental design populate the same areas.
           Few outliers are well explained by their actual structural properties, as visualized with the slice-based representation.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div class="columns is-centered">
        <img src="static/images/UMAP_fine.png" alt="UMAP, fine-grained target" style="height: 50vh"/>
        </div>
        <h2 class="subtitle has-text-centered">
         The same analysis is performed on a best-performing fine-grained model. The observations largely follow those from the coarse-grained analysis. 
         The relative placement of the clusters seems to be even more ordered than in the previous examples, which may result from the need to detect subtler differences between samples and the absence of regularizing constraints imposed by the 25 coarse-grained classes.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->
>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
